
Criar ambiente Virtual : conda create --name python2 python=2.7 -> Cria um ambiente virtual com python 2.7
Selecionar o ambiente: activate python2;
deselecionar o ambiente: deactivate;

info --envs -> Mostra todos os ambientes virtuais

-------------------------------------------

PYTHON COMANDOS

TYPE() - TIPO DA VARIAVEL

var = 'nome _variavel'
idade = 25 
('rodrigo: {one} e tenho {two} anos'.format(one=var,two=idade))

							LISTA


lista = [1.0, 'Hello', [1, 2, 30]]
s = lista[:2] -> O colchete com : seleciona parte da lista

String = 'nome_de_variavel'

split -> divide a string
map -> passa em todas posições
filter -> Caso for verdade pega o valor
-----------------------------------------------------------------------------------------------------------------
NUMPY

np.array(lista) -> cria o array ou matrizes de n dimensões
np.arange(inicio,fim,de quanto em quanto) -> cria arrays de numeros Ranger
np.zeros(qtd) -> cria um array de n-elementos zerados
np.eye(qtd) -> Cria uma matriz endentidade
np.linspace(inicio,fim,qr exibir quantos valores da lista)
np.random.rand(qtd valores aleatorios) -> Distribuição uniforme
np.random.randn(qtd valores aleatorios) -> Distribuição 
np.random.randint(0,100,10) -> Cria um array de valore aleatorio de 0 a 100
np.round(np.random.rand(5)*100,0) -> arredonda primeiro parametro o valor o segundo casa decimal
np.unique(df['col2']) -> Trás resultados unicos da coluna
arr.reshape((5,5)) -> Pega o array e o transforma exemplo matriz de 5x5
arr.shape -> Tamanho do Array ou matriz - Não tem parentes pois é parametro/atributo
arr.max -> maior valor do array
arr.min -> menor valor
arr.argmax() - > retorna o indice da posição do maior valor
arr.argmin() - > retorna o indice da posição do menor valor
arr[:2][:2] -> Recorta um Array
arr2 = arr[:] -> Faz um apontamento para o arr, mudando o arr2 muda o arr para não ocasionar utiliza o .copy()
arr2 = arr[:3].copy()
arr[1:4,5:] -> Antes da virgula seleciona linha dps coluna 
bol = arr > 40 pega os valores maior que 40
arr[bol] -> Devolve em formato de numeros	
np.srqt(Array)
np.exp(Array) -> Exponenciaçao
np.mean(Array) -> Média do Array
np.std(Array) -> Desvio padrão
np.sin(Array) -> Senho dos elementos
np.max(Array)
np.min(Array)
mat.sum(axis=0) -> Soma de coluna 0 e 1 linha
np.random.seed(101) -> Seta os mesmo numeros em qualquer computador SEED

-----------------------------------------------------------------------------------------------------------------
PANDA

pd.Series(data = minha_lista, index=labels) -> cria algo parecido com dicionario minha_lista é sua lista e labels é oq vai ser associado
ser1 + ser2 -> Soma 2 series baseadas no INDICE 
df = pd.DataFrame(np.random.randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split()) -> Cria DataFrame - 1º Seus valores 2º Indice 3ºColumas
df['W'] -> Filtra colunas
df[['W','Z']] -> Filtra mais de uma, voce cria lista por isso 2[]
df['new'] = df['W'] + df['X'] -> Cria uma nova coluna através de uma soma de 2 colunas
df.drop('new',axis=1) -> Deletar coluna 1º nome da coluna e 2º A linha
df.drop('new',axis=1,inplace=True) -> Com Inplace ele ja apaga no arquivo.
df.loc[['A','B'],['X','Z']] -> Filtrar a tabela 1º Parametro linha, 2º a colunas Baseado nos nomes indices e colunas
df.iloc[1:4,2:] -> Filtras os dados com numpy com numeros.
bol = df > 0 -> Faz seleção com base na condição retorno true e false
df[bol] -> retorna ao inves do true e false retorna os valores da condições
df[df['W']>0]['Y'] -> faz uma condicional onde retorna os valores maior que 0 na linha do 'W' e retorna a coluna Y
df[(df['W'] > 0) & (df['Y'] > 1)] -> seleção bicondicional 
sal['JobTitle'].head(n) Retorna os N primeiros 
sum(sal[sal['Year'] == 2013]['JobTitle'].value_counts() == 1) -> o Sum admiti 0 para false e 1 para True
df['Day of Week'].map(dmap) -> muda a tabela com base no dicionário passado

MULTINÍVEIS

df.reset_index(inplace=True) -> Reseta o indices setando como array [0,1,2,3,..], para mudar realmente passa parametro inplece = True
df.set_index('Estado',inplace=True) -> Seta um novo indice para o DataFrame
hier_index = list(zip(outside,inside)) -> Associa cada item da lista na outra lista
hier_index = pd.MultiIndex.from_tuples(hier_index) -> Indice multinível que é passado como parametro para cria o data frame
df = df.DataFrame(np.random.randn(6,2),index=hier_index, columns=['A','B'])
df.loc['G1'].loc[1] -> acessar os valores deste data frame acima
df.index.names = ['Grupo','Número']
df.xs(1,level="Numero") -> Filtra sem precisar passar pelo Grupo no caso do exemplo
df['Salario'].idxmax() -> a traz a linha que tem valor maximo

AUSENTES

df.dropna(axis,thresh=x) -> Deleta a linha onde há o NaN tendo outros parâmetros com shit + tab
							thresh apaga a linha com a quantidade que passado para ele.
df.fillna(value='') -> substitui os valores ausentes por outro valor
df.fillna(method='ffill') -> substitui o valor ausentes pelo ultimo registro 

GROUPBY

group = df.groupby('Empresa') -> Agrupada com o parâmetro que é passado
group.mean() -> Com o grupo você pode realizar várias operações
group.sum()
group.describe() -> Trás informações com algumas operações feitos como média desvioPadrao max min
group.sum().loc['Sam'] -> Busca SAM neste group sum


CONCATENAR, JUNTAR E MESCLAR
PESQUISAR OLHAR DOCUMENTAÇÃO (PAR
pd.concat([df1,df2,df3])-> concatena baseado na coluna pois o axis padrão é 0
pd.merge(left,right,how="inner",on='key')-> junta os 2 DataFrame onde coluna key tem dados comuns
left.join(right,how="outer") -> Igual do SQL onde junta oq está no join e onde não estiver no left coloca NaN

OPERAÇÕES

df['col2'].unique() -> Trás os resultados da coluna não repetindo dados
df['col2'].nunique() -> Trás a quantidade de dados unicos da coluna.
df['col2'].value_counts() -> Retorna os valores únicos e a quantidade que repete
df['col3'].apply(vezes2) -> chama a função para cada elemento da col3 paracido com map (df['col2'].apply(lambda x: x*x))
del df['col2'] -> a coluna que é passada METÓDO DO PYTHON
df.sort_values(by='col2') ->  Ordena sua coluna passada no parâmetro
df.isnull() -> Mostra os valores nullos 
df.dropna() -> Apaga valores onde tem NaN apagando por padrao linha
df.pivot_table(values='D',index=['A','B'], columns=('C')) -> Funciona como tabela dinâmica excel -> Reorganiza a tabela como você quer.

ENTRADA E SAÍDA DE DADOS

df = pd.read_csv('exemplo',sep=',') -> importa o excel para seu dataFrame
df.to_csv("exemplo.csv",sep=';',decimal=',') -> Exporta o DataFrame para excel
df = pd.read_excel('Exemplo_Excel.xlsx',sheetname='Sheet1') -> importa o excel da planilha com nome 'Sheet1'
df.to_excel("exemplo_Excel.xlsx",sheet_name='sheet1') -> Exporta para a planilha com nome sheet1

CORRELAÇÃO
sal['tamanho da string'] = sal['JobTitle'].apply(len)
sal[['tamanho da string','TotalPayBenefits']].corr() -> Verifica se quanto maior a string maior o salario


----------------------------------------------------------------------------------------------------
MATPLOTLIB


%matplotlib inline -> Exibi o grafico na linha

plt.plot(x,y,'r--') -> Gera o grafico com  a cor RED 'r'  e os traços o grafico fica tracejado
plt.xlabel('Eixo X') -> muda nome no eixo x
plt.ylabel('Eixo Y') -> muda nome no eixo y
plt.title('Titulo') -> muda nome o titulo

plt.subplot(1,2,1) -> gera 2 graficos 1º Linha 2ºColuna 3ºO grafico que deseja trabalhar

fig = plt.figure()-> Cria um figura em branco
axes1 = fig.add_axes([0.1,0.1,0.8,0.8]) -> configura o tamanho da figura
axes2 = fig.add_axes([0.2,0.5,0.3,0.3]) -> configura o tamanho da figura2 1ºmargem esquerda 2º baixo 3º direita 4ºcima
axes1.plot(x,y)
axes1.set_xlabel('Eixo x')
axes1.set_title('Titulo')
axes2.plot(y,x)

fig,ax = plt.subplots()-> Instancia um objeto da classe matplotlib
ax.plot(x,x**3,'b--')
ax.plot(x,x**4,'g-.')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Titulo')

fig,ax = plt.subplots(1,2) -> gera um Array de objetos

fig,ax = plt.subplots(5,5)
plt.tight_layout()-> Não deixa os graficos ficar sobrepostos


fig = plt.figure(figsize=(50,20),dpi=400) -> nao entendi pois o metodo n afeta em nada

fig,axes = plt.subplots(figsize=(12,8)) -> Altera o tamanho da figura
axes.plot(x,y,'r-')
axes.set_title('titulo')
fig.savefig("imagem.png") -> salva a sua figura nos formatos png jpg gif

axes.plot(x,y,'r-',label='x ^2') -> parametros labels adiciona legenda
axes.legend(loc=0) -> Inserir legendas 1º posição da legenda no 0 deixa a lib escolher a melhor forma (menos dados em volta)

ax.plot(x,x**2,color='#000000',linewidth=10,alpha=0.8,linestyle=':') -> linewidth largura da linha , alpha transparência, linestyle tipo da linha
ax.plot(x,x**3,'r--')

ax.set_xlim([0,2]) -> limite dos eixos
ax.set_ylim([0,10]) ->mite dos eixos


---------------------------------------------------------------------------------------------------------------

SEABORN

PLOTS DE DISTRIBUIÇÃO
tips = sns.load_dataset('tips') -> Importa nosso dataframe
sns.distplot(tips['total_bill'],kde=False,bins=60) -> Gera uma distribução com uma variavel 1º dados, 2º linha do grafico, 3ºQtd de barras
sns.jointplot(x='total_bill',y='tip',data=tips,kind='hex') -> Gera um histograma com 2 varias se tem relação, KIND -> Gera o tipo dele 'hex' um hexagonos com com cores mais forte com maior população, reg uma reta linear
sns.pairplot(tips,hue='sex',palette='coolwarm') -> Gera um grafico com todas as possibilidades numericas, HUE cria uma analise dados com base em alguma coluna de texto exemplo Sexo.
sns.rugplot(tips['total_bill']) -> Gera um traço para cada dado passado.
sns.kdeplot(tips['total_bill']) -> KDE gera uma soma da distribuição normal centrada em seus dados. 

PLOTS CATEGÓRICOS
sns.barplot(x='sex',y='total_bill',data=tips,estimator=np.std) -> Gera um grafico onde mostra a media de cada sexo no total de compra, colocando o estimator
																tem se o desvio padrão ao invés da media.
sns.countplot(x='sex',data=tips)-> Mostra a quantidade de cada informação.
sns.boxplot(x='day',y='total_bill',data=tips, palette='rainbow',hue='sex') -> um grafico onde tem se a 1º barra a 2º 25% dos dados 2º a 3º 50% dos dados, 3º a 4 75% dos dados e 4º total dos dados.
																			com o hue irá quebrar os boxplot em 2 fem e masc
  sns.boxplot(data=tips, palette='rainbow',orient='h') ->  Boxplot na horizontal
  sns.violinplot(x='day',y='tip',data=tips,hue='sex',split=True) -> Gera um KDE com o boxplot mostrando a dispersão dos dados. split cria o lado esquerdo é fem e o direito masculino
  sns.stripplot(x='day',y='total_bill',data=tips, jitter=True, hue='sex', split=True) -> forma uma linha de dados com bolas mas podendo ficar em cima da outra										
																					com jitter os ponto fica um pouco do lado, split quebra os fem e masc
sns.swarmplot(x='day',y='total_bill',data=tips, hue='sex',split=True) -> Mostra os dados um pouco mais separados que o stripplot.

sns.swarmplot(x='day',y='total_bill',data=tips,color='black')
sns.violinplot(x='day',y='total_bill',data=tips) -> com os 2 plots dando uma visão melhor
sns.factorplot(x='sex',y='total_bill',data=tips,kind='bar') -> assume qualquer dos graficos acima com o parametro kind
																
sns.heatmap(crr, cmap='coolwarm', annot=True)-> Mostra a correlação entre as variáveis em formato de mapa de calor, 1º sua correlação, 2ºEsquema de cores, 3ºMostra valores dentro dos quadrados
pf = fights.pivot_table(values='passengers',index='month',columns='year') -> Reorganiza a tabela com metodo do Pandas para melhor análise o value sempre é numerico
sns.heatmap(pf,cmap='magma',linecolor='gray',linewidth=1) -> 2ºCor da linha que divide os quadrados, 3º a espessura da linha.
sns.clustermap(pf,standard_scale=1) -> Faz um agrupamento dos dados Pesquisar mais sobre a biblioteca, 2º scala das cores.


sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',col='day',aspect=0.6,size=5) -> 'COL' coloca cada dia em um grafico, 'ASPECT' Muda o aspecto do grafico	'SIZE' Tamanho do grafico
sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',col='sex',row='time') -> 'ROW' Coloca esse dados em linhas
sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',markers=['o','v'],scatter_kws={'s':50}) -> 'MARKERS' Substitui os pontos por outra representação
																									'SCATTER_KWS' passa um dic 'S' de Size dando zoom e tirando

PAIR GRIDS

g = sns.PairGrid(iris) -> cria todos os graficos possiveis de dispersão de dados vazios
g.map(plt.scatter) -> Com essa linha mostra as dispersão entre todas as variaveis -> Parecido com PairPlot

g = sns.PairGrid(iris)
g.map_diag(plt.hist) -> Gera Graficos de barras (Histograma na diagonal
g.map_upper(plt.scatter) -> transforma a diagonal de cima em grafico de dispersão bolinhas
g.map_lower(sns.kdeplot) -> Transforma a diagonal de baixo em grafico de kde com o parametro sns.kdeplot

g = sns.FacetGrid(tips,col='time',row='smoker') -> Pega seu dados CATEGORICOS, fala q a coluna é time é a linha smoker
g.map(plt.hist,'total_bill') -> monta o gráfico em função do 'total_bill'
sns.set_style('whitegrid') muda fundo do grafico

ESTILOS E CORES


sns.set_style('ticks')-> Muda o estilo do grafico o fundo a borda
sns.countplot(x='sex',data=tips) -> monta o grafico com quantidade de variavel
sns.despine()-> Posso tirar o contorno de qualquer lado do grafico
plt.figure(figsize=(12,8)) -> COmo o SEABORN faz parte do matplotlib ele pode usar o metodo para alter tamanho da figura
sns.lmplot(x='total_bill',y='tip',data=tips,size=2,aspect=15) -> Regressão Linear podendo mudar os aspecto com size e aspect
sns.set_context('poster',font_scale=1) -> muda o estilo do grafico podendo aumentar a fonte
sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',palette='inferno) -> Pallete esquema de cores que pode se usado ao pesquisar em https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html



VISUALIZAÇÃO DE DADOS PANDAS

df1['A'].hist() -> plota um histograma ficando mais bonito importando seaborn
df2.plot.bar(stacked=True) -> Gera um grafico de barras com o parametro Stacked ele junta tudo na mesma barra
df1.plot.line(x=df1.index,y='B',figsize=(12,5),lw=1) -> Forma um grafico de linha.
df1.plot.scatter(x='A',y='B',s=df1['C']) -> Com essa linha mostra as dispersão entre todas as variaveis entre as variaveis A e B e dependendo do valor C muda a cor das bolinhas
df2.plot.box() -> Faz um box ploat
df.plot.hexbin(x='A',y='B',gridsize=20,cmap='Oranges') -> Cria um grafico hexagonal 
df.plot.kde() -> Mostra o kde de todas variaveis
df2.plot.area(alpha=0.4) -> plota um grafico de areas
--- tira legenda do quadro
fig = plt.figure()
sns.countplot(data=df,x='Day of Week',hue='Reason')
plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
dayHour = df.groupby(by=['Day of Week','Hour']).count()['Reason'].unstack() ->Reorganiza sua matriz multnivel


--------------------------------------------------------------------------------------------

PLOTLY E CUFFLINKS

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import cufflinks as cf
init_notebook_mode(connected=True)
cf.go_offline()

df.iplot(kind='scatter',x='A',y='B',mode='markers') -> Gera um grafico de dispersão dos dados, mode p ficar de bolinhas
df.iplot(kind='box') -> cria boxplot
df2.iplot(kind='bar',x='Categoria',y='Valores') -> grafico de barras
df3.iplot(kind='surface',colorscale='rdylbu') -> gera um grafico com 3DIMENSÕES 
df[['A','B']].iplot(kind='spread') -> Análise de variaveis temporais PESQUISAR MAIS
df['A'].iplot(kind='hist',bins=50) -> Gera im histograma
df.iplot(kind='bubble',x='A',y='B', size='C') -> Gráfico de bolha size varia com a variavel C
df.scatter_matrix() -> Gerá um grafico de matriz como SEABORN PESQUISAR




------------------------------------------------------------------------------------------
MACHINE LEARNING

Regressões Lineares -> ler capitulo 2 e 3
kaggle -> Dados reais da msm linha

retorna 4 variaveis como resposta sendo test e train
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4, random_state=101) -> Realiza divisão dos dados em duas partes passo x var parametros e y resultado final, test_size = 40% dos dados serão teste , random_state o algoritmo q irafazer a divisão

from sklearn.linear_model import LinearRegression importa a bib de regressa linear
lm = LinearRegression() cria um objeto dela
lm.fit(x_train,y_train) -> função fit Encontra os parametros linear baseado no x e y train

from sklearn import metrics
print('MAE',metrics.mean_absolute_error(y_test,predict))
print('MSE',metrics.mean_squared_error(y_test,predict))
print('RMSE',np.sqrt(metrics.mean_squared_error(y_test,predict)))

Balanço Viés-Variância ler capitulo 2
Teoria

Regressão Logística Seção 4 a 4.3
Classificações como se um email é ou não spam
Utiliza-se uma função logística conhecida como sigmoide
Matriz de Confusão -> método para avaliar o resultado
pega todos True e soma e divide o total e o resultado da entre 0 e 1

pd.get_dummies(train['Sex'],drop_first=True) -> pega coluna Categorica como sexo e transforma em 0 e 1 drop_first apaga a primeira coluna


K NEAREST NEIGHBORS (KNN) Seção 4.6 introduction to statistical Learning
from sklearn.preprocessing import StandardScaler método de padronização -> Pois os valores´podem ter pesos assim ficam todos no msm nivel
scaler = StandardScaler() instancia um objeto da classe
scaler.fit(df.drop('TARGET CLASS',axis=1)) seleciona os dados para ser treinados
df_normalizado = scaler.transform(df.drop('TARGET CLASS',axis=1)) -> Realiza o processo de normalização dos dados

df_param = pd.DataFrame(df_normalizado,columns=df.columns[:-1]) -> cria um DataFrame com os dados normalizados

from sklearn.model_selection import train_test_split ->Treinar o algoritmo

x_train, x_test, y_train, y_test = train_test_split(df_param,df['TARGET CLASS'],test_size=.3) -> Fazendo as divisões dos dados

from sklearn.neighbors import KNeighborsClassifier Biblioteca para classificar

knn = KNeighborsClassifier(n_neighbors=1) -> instancio a classe e passo como parametro a qtd de vizinhos

knn.fit(x_train,y_train) treinar os dados

pred = knn.predict(x_test) testar a previsao

from sklearn.metrics import classification_report,confusion_matrix biblioteca para ver o resultado

print(classification_report(y_test,pred))

print(confusion_matrix(y_test,pred))

ARVORES DE DECISÃO E FLORESTAS ALEATÓRIAS SEÇÃO 8 DO INTRODUTION

analisa a linha na horizontal
 avores aleatorias
 x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.3) monta o modelo
 
 from sklearn.tree import DecisionTreeClassifier
 dtree = DecisionTreeClassifier()
 dtree.fit(x_train,y_train)
 pred = dtree.predict(x_test)
 from sklearn.metrics import classification_report, confusion_matrix
 print(classification_report(y_test,pred))
 
 FLORESTAS ALEATORIA
 from sklearn.ensemble import RandomForestClassifier
 RFC = RandomForestClassifier(n_estimators=200) -> parametro qtd de arvores aleatotias
 
 rfc.fit(x_train,y_train)
	rfc_pred = rfc.predict(x_test)

SVM SEÇÃO 9 DO INTRODUTION

Tenta separa os seus dados maximizando as margens dos vetores de suporte (Cria um plano linearmente)
kernel trick transforma os planos dos dados
from sklearn.datasets import load_breast_cancer importação de dados reaia relacionado ao cancer
cancer = load_breast_cancer()
cancer.keys() -> descrição

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(df_cancer,np.ravel(df_target),test_size=0.3)

from sklearn.svm import SVC

model = SVC()

model.fit(x_train,y_train)

pred = model.predict(x_test)

print(classification_report(y_test,pred))
print('\n')
print(confusion_matrix(y_test,pred)) 

->>>>>>>>>> Resultado não é o melhor não temos conhecimentos dos parametros com utilaza um metodo q testa tds possibilidades 

param_grid = {'C':[0.1,1,10,100,100],'gamma':[1,0.1,0.001,0.0001],'kernel':['rbf']} 
C -> Controla custo de classificações errada sempre fitando os dados de train
gama -> Gaussiana com alta variancia, 

from sklearn.model_selection import GridSearchCV

 grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)

grid.fit(x_train,y_train)

grid.best_params_

pred = grid.predict(x_test)

print(classification_report(y_test,pred))
print('\n')
print(confusion_matrix(y_test,pred))

 K Means Clustering -> SEÇÃO 10 DO INTRODUTION
 Separa os dados em grupos 
 from sklearn.cluster import KMeans
 
 kmeans = KMeans(n_clusters=2) -> quantidade de grupos 
 kmeans.fit(df.drop('Private',axis=1))
 kmeans.cluster_centers_
 kmeans.labels_
 
 
ANÁLISE DE COMPONENTE PRINCIPAL (PCA) SEÇÃO 10.2 DO INTRODUTION AULAS DE ALGEBRA LINEAR
from sklearn.datasets import load_breast_cancer importa dados do proprio sklearn
cancer = load_breast_cancer()
cancer.keys()
df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])

from sklearn.preprocessing import StandardScaler Padroniza seus dados em uma escala
scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)


from sklearn.decomposition import PCA Treinamento do modelo PCA
pca = PCA( n_components=2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)
x_pca.shape

PLOTA OS DADOS NO GRAFICO
sns.set_style('whitegrid')
plt.figure(figsize=(12,8))
plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')
plt.xlabel('Primeiro componentes principal')
plt.ylabel('Segundo componentes principal')


Teoria do processamento de linguagem natural PLN ou NLP LER MAIS
 Modelo de saco de palavras
 
 import nltk
 nltk.download() -> baixar a biblioteca stopwords onde retira-se pronomes,artigo, para não gerar ruidos nos modelos
 messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')] ->Limpa o texto com comando rstrip() tirando .,
 
 for messagem_number,message in enumerate(messages[:15]): -> Gera uma coluna de index pros textos
    print(messagem_number,message)
    print('\n')
 
 messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\t',names=['label','message']) -> Cria data framw
 messages['lenght'] = messages['message'].apply(len) -> Cria uma coluna com tamanho de cada linha 
 
 import string 
 sempont = [car for car in mess if car not in string.punctuation] -> remove as pontuações da palavra transformando em lista de letras
 sempont = ''.join(sempont) -> Join ele junta as palavras baseados no q passa de parametro nas aspas

 from nltk.corpus import stopwords -> Biblioteca Stopwords
 stopwords.words('english') -> contém uma lista de todas palavras onde não importa na frase
 clean_mess = [word for word in tst.split() if word.lower() not in stopwords.words('english') ] -> Remove palavras inutéis
 
 def txt_process(mess): -> Função onde remove pontuações e palavras inutéis
    #Retira pontuações
    nopunc = [char for char in mess if char not in string.punctuation]
    
    #juntar novam. para formar as Strings
    nopunc = ''.join(nopunc)
    
    #Remover as Stop Words
    sms = [word for word in nopunc.split() if word not in stopwords.words('english')]
    
    return sms
	
from sklearn.feature_extraction.text import CountVectorizer -> Biblioteca matriz esparça (Contém mt Zeros)
bow_transformer = CountVectorizer(analyzer=txt_process).fit(messages['message']) -> Cria nossa matriz esperça
print(len(bow_transformer.vocabulary_)) -> Total de palavras existentes

message4 = messages['message'][3]
print(message4) -> Pega palavra do df
bow4 = bow_transformer.transform([message4]) -> mostra a msm com o bow_transformer



from sklearn.feature_extraction.text import CountVectorizer
countVectorizer = CountVectorizer()

X = countVectorizer.fit_transform(X)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=101)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train,y_train)
pred = nb.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.ensemble import RandomForestClassifier

COM TFIDF
pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_process)),
    ('tfidf',TfidfTransformer()),
    ('classifier', MultinomialNB()),
])

X = yelp_class['text']
y = yelp_class['stars']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=101)
pipeline.fit(X_train,y_train)
pred = pipeline.predict(X_test)
print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))